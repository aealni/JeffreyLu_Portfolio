<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500&display=swap" rel="stylesheet">
</head>
<header id="header-placeholder" class="w-full bg-red-500 text-white p-6">
    <script>
        const headerTitle = "Stochastic Gradient Descent";

        fetch("/components/nav.html")
            .then(res => res.text())
            .then(html => {
                document.getElementById("header-placeholder").innerHTML = html;
                document.getElementById("page-title").textContent = headerTitle;
                const script = document.createElement('script');
                script.src = "/scripts/menu.js";
                document.body.appendChild(script);
            });
    </script>
</header>


<body class="bg-gray-50 font-sans antialiased">

    <main class="mt-12 px-4 mb-6">
        <div class="max-w-2xl mx-auto bg-white p-8 rounded-lg shadow">

            <p class="mb-4"> This project was developed as part of CS 361: Probability and Statistics for Computer Science at the University of Illinois Urbana-Champaign. The focus was on exploring optimization algorithms that are foundational to modern machine learning, with an emphasis on gradient-based methods for improving model performance. </p> 
            <p class="mb-4"> Gradient descent is one of the most fundamental tools for training machine learning models. It works by iteratively adjusting model parameters to minimize a loss function — essentially, the mathematical measure of how far off the model’s predictions are from the correct outputs. You can picture it like hiking downhill in dense fog: you can’t see the entire landscape, but by feeling the slope beneath your feet - following the gradient — you take steps toward the lowest point, where the model ideally performs best. </p> 
            <p class="mb-4"> Traditional or "batch" gradient descent calculates the gradient using the entire dataset before making each update. While effective for small datasets, this approach becomes computationally expensive and slow as dataset size increases — a common scenario in real-world machine learning tasks. </p> 
            <p class="mb-4"> To address this, Stochastic Gradient Descent (SGD) introduces randomness by using only a single example — or a small, randomly selected batch — to estimate the gradient and update parameters. This significantly reduces computation per iteration and introduces variability that can help the optimizer escape shallow local minima, improving the chances of finding better solutions. </p> 
            <p class="mb-4"> However, SGD isn’t without its challenges. Noisy updates and the sensitivity of learning rates can slow down training or cause unstable convergence. To overcome these limitations, more advanced optimizers like Adam (Adaptive Moment Estimation) have been developed. Adam extends SGD by incorporating momentum and adaptive learning rates, maintaining moving averages of past gradients and their squared values - if you’ve been consistently taking steps downhill on that foggy hill, Adam lets you pick up speed in that direction. It recognizes when the slope has been pointing the same way for a while and accelerates progress. Conversely, if the terrain is uneven or changes direction frequently, Adam slows things down to prevent overshooting. This adaptive behavior combines the benefits of momentum (which smooths updates) and per-parameter learning rates, making training more stable and efficient. </p> 
            <p class="mb-4"> Nowadays, Adam is widely used across deep learning applications due to its fast convergence and ability to handle complex, high-dimensional optimization landscapes. Its adaptive mechanisms reduce the need for extensive hyperparameter tuning, making it sort of the foundation for optimizers beyond the classic gradient descent methodologies. </p>
            <div class="flex justify-center items-center space-x-2 mt-6">
                <a href="/assets/CS361_Project_SP25.pdf" target="_blank" class="bg-blue-600 text-white py-2 px-4 rounded-md hover:bg-blue-500 transition duration-300">Project Prompt</a>
                <a href="/assets/StochasticGradientDescent.pdf" target="_blank" class="bg-orange-500 text-white py-2 px-4 rounded-md hover:bg-blue-700 transition duration-300">View Writeup</a>
                <a href="/assets/sgd_code.pdf" target="_blank" class="bg-red-500 text-white py-2 px-4 rounded-md hover:bg-blue-700 transition duration-300">View Code</a>
            </div>
        </div>
    </main>

</body>

</html>